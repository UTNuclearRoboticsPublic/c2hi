/**:
  ros__parameters:

    config_name: best_campus_vis_role_artag_cmd

clip_vis_rec_server:
  ros__parameters:
    clip_model: 'RN50'
    object_classes: ['person']

    person:
      attributes:
        variables: ['']

      states:
        variables: ['role']
        role:
          labels: ['pedestrian','teammate','supervisor']
          descriptions: ['a picture of a person',
                         'a picture of a person wearing an orange texas robotics t-shirt and an orange texas hat',
                         'a picture of a person wearing an orange texas robotics t-shirt and an orange texas hat and a yellow safety vest']
      
      comms:
        labels: ['']
        gesture_descriptions: ['']
        probs: []
        # p(command_detected|command_issued)
        gesture_sensor_model_coeffs: []        
        verbal_sensor_model_coeffs: []

### MULTIMODAL PROCESSING - SEMANTIC FUSION NODE
/semantic_fusion_node:
  ros__parameters:

    role_rec_methods: ['visual']
    command_rec_methods: ['artag']

    sensors:
      clip_role_rec:
        type: 'vision'
        topic: ''

        update_method: 'time'
        update_threshold: .9 # 

        # GTSAM fusion variables
        role_obs_labels: ['pedestrian','teammate','supervisor']
        role_obs_model_coeffs: [9815.0,1673.0,19.0,
                                205.0,12823.0,3.0,
                                10.0,1.0,11332.0]
